{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pathlib\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.policy_model.policy_model_utils import (load_policy_model, get_policy_probs, create_data_range_dict,\n",
    "                                                 compute_next_step_reconstruction, compute_scores)\n",
    "from src.reconstruction_model.reconstruction_model_utils import load_recon_model\n",
    "from src.helpers.data_loading import create_data_loader\n",
    "from src.helpers.torch_metrics import compute_ssim, compute_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 'knee'  # or 'brain'\n",
    "# wandb_entity = 'timsey'  # 'WANDB_ENTITY NAME'\n",
    "# wandb_project = 'mri_refactor'  # 'WANDB_PROJECT_NAME'\n",
    "\n",
    "# # Whether to overwrite SSIM values stored on drive if exist\n",
    "# force = False\n",
    "\n",
    "# # Set base path for policy models.\n",
    "# base_policy_path = '/home/timsey/Projects/mrimpro/refactor'\n",
    "\n",
    "# if dataset.lower() == 'knee':\n",
    "#     # Set data path, recon model checkpoint, and policy model checkpoints\n",
    "#     data_path = '/home/timsey/HDD/data/fastMRI/singlecoil/'\n",
    "#     recon_model_checkpoint = '/home/timsey/Projects/fastMRI-shi/models/unet/al_nounc_res128_8to4in2_cvol_symk/model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN VALUES TO SET ###\n",
    "\n",
    "dataset = 'Knee'  # or 'Brain'\n",
    "entity = 'WANDB_ENTITY NAME'\n",
    "wandb_project = 'WANDB_PROJECT_NAME'\n",
    "\n",
    "force = False\n",
    "\n",
    "# Set base path for policy models. Corresponds to exp_dir in train_policy.py\n",
    "base_policy_path = '<base_path_to_stored_models>'\n",
    "\n",
    "if dataset.lower() == 'knee':\n",
    "    # Set data path, recon model checkpoint, and policy model checkpoints\n",
    "    data_path = '<path_to_knee_data>'\n",
    "    recon_model_checkpoint = '<path_to_recon_model.pt>'\n",
    "    \n",
    "elif dataset.lower() == 'brain':\n",
    "    # Set data path, recon model checkpoint, and policy model checkpoints\n",
    "    data_path = '<path_to_brain_data>'\n",
    "    recon_model_checkpoint = '<path_to_recon_model.pt>'\n",
    "    \n",
    "### END VALUES TO SET ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a dictionary of run names, dirs and ids, based on the Wandb API.\n",
    "\n",
    "if dataset == 'knee':\n",
    "    sample_rate = 0.5\n",
    "if dataset == 'brain':\n",
    "    sample_rate = 0.2\n",
    "\n",
    "run_id_dict = {\"16-32\": defaultdict(dict),\n",
    "               \"4-32\": defaultdict(dict)}\n",
    "\n",
    "runs = api.runs(f\"{wandb_entity}/{wandb_project}\", {\"config.sample_rate\": sample_rate})\n",
    "for run in runs:\n",
    "    if not run.state == 'finished':\n",
    "        continue\n",
    "    \n",
    "    name = run.name\n",
    "    args = run.config\n",
    "    \n",
    "    if args['dataset'].lower() != dataset.lower():\n",
    "        continue  # Skip models not on given dataset\n",
    "        \n",
    "    ### YOUR FILTERS HERE ###\n",
    "\n",
    "    if args['model_type'] == 'greedy':\n",
    "        key = 'greedy'\n",
    "    elif args['gamma'] == 1:\n",
    "        key = 'nongreedy'\n",
    "    else:\n",
    "        key = args['gamma']\n",
    "                \n",
    "    run_dir = args['run_dir'].split('/')[-1]\n",
    "    \n",
    "    if args['accelerations'] == [8]:\n",
    "        run_id_dict[\"16-32\"][key][name] = {'id': run.id, 'dir': run_dir}\n",
    "    elif args['accelerations'] == [32]:\n",
    "        run_id_dict[\"4-32\"][key][name] = {'id': run.id, 'dir': run_dir}\n",
    "            \n",
    "pprint(run_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_dict = defaultdict(lambda: defaultdict(dict))\n",
    "psnr_dict = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for horizon, gamma_dict in run_id_dict.items():\n",
    "    for gamma, id_dict in gamma_dict.items():\n",
    "        ssim_dict[horizon][gamma] = defaultdict()\n",
    "        psnr_dict[horizon][gamma] = defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckpt_from_id(run_id, base, entity, project):\n",
    "    run = api.run(f'{entity}/{project}/{run_id}')\n",
    "    args = run.config\n",
    "    ckpt = base / pathlib.Path(args['run_dir']).name / 'model.pt'\n",
    "    return ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, recon_model, model, loader, data_range_dict):\n",
    "    \"\"\"\n",
    "    Evaluates using SSIM of reconstruction over trajectory. Doesn't require computing targets!\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ssims, psnrs = 0, 0\n",
    "    tbs = 0  # data set size counter\n",
    "    with torch.no_grad():\n",
    "        for it, data in enumerate(loader):\n",
    "            kspace, masked_kspace, mask, zf, gt, gt_mean, gt_std, fname, _ = data\n",
    "            # shape after unsqueeze = batch x channel x columns x rows x complex\n",
    "            kspace = kspace.unsqueeze(1).to(args.device)\n",
    "            masked_kspace = masked_kspace.unsqueeze(1).to(args.device)\n",
    "            mask = mask.unsqueeze(1).to(args.device)\n",
    "            # shape after unsqueeze = batch x channel x columns x rows\n",
    "            zf = zf.unsqueeze(1).to(args.device)\n",
    "            gt = gt.unsqueeze(1).to(args.device)\n",
    "            gt_mean = gt_mean.unsqueeze(1).unsqueeze(2).unsqueeze(3).to(args.device)\n",
    "            gt_std = gt_std.unsqueeze(1).unsqueeze(2).unsqueeze(3).to(args.device)\n",
    "            unnorm_gt = gt * gt_std + gt_mean\n",
    "            data_range = torch.stack([data_range_dict[vol] for vol in fname])\n",
    "            tbs += mask.size(0)\n",
    "\n",
    "            # Base reconstruction model forward pass\n",
    "            recons = recon_model(zf)\n",
    "            unnorm_recons = recons[:, :, :, :] * gt_std + gt_mean\n",
    "            init_ssim_val = compute_ssim(unnorm_recons, unnorm_gt, size_average=False,\n",
    "                                         data_range=data_range).mean(dim=(-1, -2)).sum()\n",
    "            init_psnr_val = compute_psnr(args, unnorm_recons, unnorm_gt, data_range).sum()\n",
    "\n",
    "            batch_ssims = [init_ssim_val.item()]\n",
    "            batch_psnrs = [init_psnr_val.item()]\n",
    "\n",
    "            for step in range(args.acquisition_steps):\n",
    "                policy, probs = get_policy_probs(model, recons, mask)\n",
    "                if step == 0:\n",
    "                    actions = torch.multinomial(probs.squeeze(1), args.num_test_trajectories, replacement=True)\n",
    "                else:\n",
    "                    actions = policy.sample()\n",
    "                # Samples trajectories in parallel\n",
    "                # For evaluation we can treat greedy and non-greedy the same: in both cases we just simulate\n",
    "                # num_test_trajectories acquisition trajectories in parallel for each slice in the batch, and store\n",
    "                # the average SSIM score every time step.\n",
    "                mask, masked_kspace, zf, recons = compute_next_step_reconstruction(recon_model, kspace,\n",
    "                                                                                   masked_kspace, mask, actions)\n",
    "                ssim_scores, psnr_scores = compute_scores(args, recons, gt_mean, gt_std, unnorm_gt, data_range,\n",
    "                                                          comp_psnr=True)\n",
    "                assert len(ssim_scores.shape) == 2\n",
    "                ssim_scores = ssim_scores.mean(-1).sum()\n",
    "                psnr_scores = psnr_scores.mean(-1).sum()\n",
    "                \n",
    "                # eventually shape = al_steps\n",
    "                batch_ssims.append(ssim_scores.item())\n",
    "                batch_psnrs.append(psnr_scores.item())\n",
    "\n",
    "            # shape of al_steps\n",
    "            ssims += np.array(batch_ssims)\n",
    "            psnrs += np.array(batch_psnrs)\n",
    "\n",
    "    ssims /= tbs\n",
    "    psnrs /= tbs\n",
    "\n",
    "    return ssims, psnrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self, dataset, recon_model_checkpoint, policy_model_checkpoint, data_path, accel=8, acq=16):\n",
    "        self.seed = 0\n",
    "        self.device = 'cuda'\n",
    "        self.num_workers = 8\n",
    "        self.acquisition = None\n",
    "        self.reciprocals_in_center = [1]\n",
    "        self.recon_model_checkpoint = pathlib.Path(recon_model_checkpoint)\n",
    "        self.policy_model_checkpoint = pathlib.Path(policy_model_checkpoint)\n",
    "        self.data_path = pathlib.Path(data_path)\n",
    "        self.accelerations = [accel]\n",
    "        self.center_fractions = [1 / accel]\n",
    "        self.acquisition_steps = acq\n",
    "        self.num_trajectories = 8\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if dataset.lower() == 'knee':\n",
    "            self.center_volume = True\n",
    "            self.sample_rate = 0.5\n",
    "            self.val_batch_size = 512\n",
    "            self.resolution = 128\n",
    "        elif dataset.lower() == 'brain':\n",
    "            self.center_volume = False\n",
    "            self.sample_rate = 0.2\n",
    "            self.val_batch_size = 128\n",
    "            self.resolution = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recon model\n",
    "args = Arguments(dataset, recon_model_checkpoint, 'None', data_path)\n",
    "recon_args, recon_model = load_recon_model(args)\n",
    "\n",
    "for horizon, mode_dict in run_id_dict.items():\n",
    "    if horizon == '16-32':\n",
    "        accel = 8\n",
    "        acq = 16\n",
    "    elif horizon == '4-32':\n",
    "        accel = 32\n",
    "        acq = 28\n",
    "    else:\n",
    "        raise ValueError()\n",
    "                \n",
    "    # Load data for this horizon\n",
    "    args = Arguments(dataset, recon_model_checkpoint, 'None', data_path, accel, acq)\n",
    "    loader = create_data_loader(args, 'test')\n",
    "    data_range_dict = create_data_range_dict(args, loader)\n",
    "\n",
    "    for mode, runs in mode_dict.items():     \n",
    "        for name, run_info in runs.items():\n",
    "            run_id = run_info['id']\n",
    "            run_dir = run_info['dir']\n",
    "            ckpt = get_ckpt_from_id(run_id, pathlib.Path(base_policy_path), wandb_entity, wandb_project)\n",
    "            assert str(ckpt.parent.name) == run_dir, 'Something went wrong with storing directory.'\n",
    "            \n",
    "            try:\n",
    "                model, policy_args = load_policy_model(pathlib.Path(ckpt))\n",
    "            except FileNotFoundError:\n",
    "                print(f' File corresponding to {name} not found:\\n    {ckpt}')\n",
    "                continue\n",
    "\n",
    "            policy_args.num_test_trajectories = args.num_trajectories\n",
    "            ssim_save_path = pathlib.Path(ckpt).parent / f'test_ssims_t{args.num_trajectories}.pkl'\n",
    "            psnr_save_path = pathlib.Path(ckpt).parent / f'test_psnrs_t{args.num_trajectories}.pkl'\n",
    "\n",
    "            if ssim_save_path.exists() and psnr_save_path.exists() and not force:\n",
    "                print(f'SSIMs already stored in: {ssim_save_path}')\n",
    "                with open(ssim_save_path, 'rb') as f:\n",
    "                    ssims = pickle.load(f)\n",
    "                print(f'PSNRs already stored in: {psnr_save_path}')\n",
    "                with open(psnr_save_path, 'rb') as f:\n",
    "                    psnrs = pickle.load(f)\n",
    "            else:\n",
    "                ssims, psnrs = evaluate(policy_args, recon_model, model, loader, data_range_dict)\n",
    "                \n",
    "            ssim_dict[horizon][mode][name] = [ssims, run_dir]\n",
    "            with open(ssim_save_path, 'wb') as f:\n",
    "                pickle.dump(ssims, f)\n",
    "                \n",
    "            psnr_dict[horizon][mode][name] = [psnrs, run_dir]\n",
    "            with open(psnr_save_path, 'wb') as f:\n",
    "                pickle.dump(psnrs, f)\n",
    "            \n",
    "            print(name, run_id, ssims[0], ssims[-1])\n",
    "            print(name, run_id, psnrs[0], psnrs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(score_dict):\n",
    "    ready_best_dirs = []\n",
    "    for horizon, hor_dict in score_dict.items():\n",
    "        for mode, mode_dict in hor_dict.items():\n",
    "            end_scores = []\n",
    "            run_dirs = []\n",
    "            for name, (ssims, run_dir) in mode_dict.items():\n",
    "                end_scores.append(ssims[-1])\n",
    "                run_dirs.append(run_dir)\n",
    "\n",
    "            if len(end_scores) == 0:\n",
    "                continue\n",
    "                \n",
    "            if len(end_scores) == 5:\n",
    "                max_ind = np.argmax(end_scores)\n",
    "                ready_best_dirs.append(run_dirs[max_ind])\n",
    "\n",
    "            end_scores = np.array(end_scores)\n",
    "            mean = end_scores.mean()\n",
    "            std = end_scores.std(ddof=1)\n",
    "\n",
    "            print(len(end_scores), f'Horizon {horizon}, model {mode}: {mean:.4f} \\pm {std:.4f}')\n",
    "    print(ready_best_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(ssim_dict)\n",
    "# print_scores(psnr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rim",
   "language": "python",
   "name": "rim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
